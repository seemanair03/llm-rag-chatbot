{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd80z9sLDQen"
      },
      "source": [
        "# Install prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQUewvG7woGc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai chromadb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-BV1o4ex6r9"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw4zLjr4--yN"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-community langchain-openai openai chromadb pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgZg70DsCSFO"
      },
      "outputs": [],
      "source": [
        "!pip uninstall langchain langchain-openai -y\n",
        "!pip install -U langchain langchain-openai openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpCwkfV1DYCD"
      },
      "source": [
        "# Full LangChain Implementation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx3vOGk6w0-o"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "#from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1CzeDkB_E_E"
      },
      "outputs": [],
      "source": [
        "# Step 1: Set up Azure OpenAI client\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "az_endpoint = \"https://your_openai_endpoint.openai.azure.com\"\n",
        "a_version = \"your-api-version\" # example - 2023-07-01-preview\n",
        "\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"your-api-key\",\n",
        "    api_version=a_version,\n",
        "    azure_endpoint=az_endpoint\n",
        ")\n",
        "\n",
        "chat_deployment = \"your-chat-model-deployment\" # example - gpt-4o-mini\n",
        "embedding_deployment = \"your-embedding-model-deployment\" # example - text-embedding-ada-002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry9q1EGh_Oa4"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a LangChain-compatible embedding wrapper\n",
        "from langchain_core.embeddings import Embeddings\n",
        "\n",
        "class AzureEmbeddingFunction(Embeddings):\n",
        "    def __init__(self, client, model):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        response = self.client.embeddings.create(\n",
        "            input=texts,\n",
        "            model=self.model\n",
        "        )\n",
        "        return [r.embedding for r in response.data]\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        response = self.client.embeddings.create(\n",
        "            input=[text],\n",
        "            model=self.model\n",
        "        )\n",
        "        return response.data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNVwmjSQJ-iG"
      },
      "outputs": [],
      "source": [
        "# Step 3: Load your PDF and create a fresh vector store\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Load new PDF\n",
        "pdf_path = \"your_document.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Create embedding function\n",
        "embedding_function = AzureEmbeddingFunction(client, model=embedding_deployment)\n",
        "\n",
        "# Create a truly fresh, in-memory vector store\n",
        "db = Chroma.from_documents(\n",
        "    documents,\n",
        "    embedding=embedding_function,\n",
        "    collection_name=\"temp_collection\",  # Use a unique name to avoid reuse\n",
        "    persist_directory=None  # No persistence\n",
        ")\n",
        "retriever = db.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t-EPQ-B_XeG"
      },
      "outputs": [],
      "source": [
        "# Step 4: Set up prompt and chat model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Use the following context to answer the question. If the answer isn't in the context, say you don't know.\"),\n",
        "    (\"user\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
        "])\n",
        "\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    api_key=\"your-api-key\"\n",
        "    azure_endpoint= \"your-chat-model-deployment-endpoint\",\n",
        "    deployment_name=\"your-chat-model-deployment\" # example - gpt-4o-mini\n",
        "    api_version=\"your-api-version\" # example - 2025-01-01-preview\n",
        ")\n",
        "\n",
        "# Build the chain\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5UqvxzY_doH"
      },
      "outputs": [],
      "source": [
        "# Step 5: Ask a question\n",
        "response = chain.invoke(\"What is the main topic of the document?\")\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
